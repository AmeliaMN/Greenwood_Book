<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="6.1 Relationships between two quantitative variables | Intermediate Statistics with R" />
<meta property="og:type" content="book" />


<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="6.1 Relationships between two quantitative variables | Intermediate Statistics with R">

<title>6.1 Relationships between two quantitative variables | Intermediate Statistics with R</title>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if bootstrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover" id="toc-cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments" id="toc-acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1" id="toc-chapter1"><span class="toc-section-number">1</span> Preface</a>
<ul>
<li><a href="1.1-section1-1.html#section1-1" id="toc-section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1.2-section1-2.html#section1-2" id="toc-section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1.3-section1-3.html#section1-3" id="toc-section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1.4-section1-4.html#section1-4" id="toc-section1-4"><span class="toc-section-number">1.4</span> Quarto</a></li>
<li><a href="1.5-section1-5.html#section1-5" id="toc-section1-5"><span class="toc-section-number">1.5</span> Grammar of Graphics</a></li>
<li><a href="1.6-section1-6.html#section1-6" id="toc-section1-6"><span class="toc-section-number">1.6</span> Exiting RStudio</a></li>
<li><a href="1.7-section1-7.html#section1-7" id="toc-section1-7"><span class="toc-section-number">1.7</span> Chapter summary</a></li>
<li><a href="1.8-section1-8.html#section1-8" id="toc-section1-8"><span class="toc-section-number">1.8</span> Summary of important R code</a></li>
<li><a href="1.9-section1-9.html#section1-9" id="toc-section1-9"><span class="toc-section-number">1.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2" id="toc-chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a>
<ul>
<li><a href="2.1-section2-1.html#section2-1" id="toc-section2-1"><span class="toc-section-number">2.1</span> Data wrangling and density curves</a></li>
<li><a href="2.2-section2-2.html#section2-2" id="toc-section2-2"><span class="toc-section-number">2.2</span> Pirate-plots</a></li>
<li><a href="2.3-section2-3.html#section2-3" id="toc-section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2.4-section2-4.html#section2-4" id="toc-section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2.5-section2-5.html#section2-5" id="toc-section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2.6-section2-6.html#section2-6" id="toc-section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2.7-section2-7.html#section2-7" id="toc-section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2.8-section2-8.html#section2-8" id="toc-section2-8"><span class="toc-section-number">2.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="2.9-section2-9.html#section2-9" id="toc-section2-9"><span class="toc-section-number">2.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2.10-section2-10.html#section2-10" id="toc-section2-10"><span class="toc-section-number">2.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2.11-section2-11.html#section2-11" id="toc-section2-11"><span class="toc-section-number">2.11</span> Chapter summary</a></li>
<li><a href="2.12-section2-12.html#section2-12" id="toc-section2-12"><span class="toc-section-number">2.12</span> Summary of important R code</a></li>
<li><a href="2.13-section2-13.html#section2-13" id="toc-section2-13"><span class="toc-section-number">2.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3" id="toc-chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a>
<ul>
<li><a href="3.1-section3-1.html#section3-1" id="toc-section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3.2-section3-2.html#section3-2" id="toc-section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell means and reference-coding)</a></li>
<li><a href="3.3-section3-3.html#section3-3" id="toc-section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3.4-section3-4.html#section3-4" id="toc-section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3.5-section3-5.html#section3-5" id="toc-section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3.6-section3-6.html#section3-6" id="toc-section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3.7-section3-7.html#section3-7" id="toc-section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="3.8-section3-8.html#section3-8" id="toc-section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3.9-section3-9.html#section3-9" id="toc-section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3.10-section3-10.html#section3-10" id="toc-section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4" id="toc-chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a>
<ul>
<li><a href="4.1-section4-1.html#section4-1" id="toc-section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4.2-section4-2.html#section4-2" id="toc-section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4.3-section4-3.html#section4-3" id="toc-section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4.4-section4-4.html#section4-4" id="toc-section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4.5-section4-5.html#section4-5" id="toc-section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4.6-section4-6.html#section4-6" id="toc-section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="4.7-section4-7.html#section4-7" id="toc-section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4.8-section4-8.html#section4-8" id="toc-section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4.9-section4-9.html#section4-9" id="toc-section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5" id="toc-chapter5"><span class="toc-section-number">5</span> Chi-square tests</a>
<ul>
<li><a href="5.1-section5-1.html#section5-1" id="toc-section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="5.2-section5-2.html#section5-2" id="toc-section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5.3-section5-3.html#section5-3" id="toc-section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5.4-section5-4.html#section5-4" id="toc-section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5.5-section5-5.html#section5-5" id="toc-section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5.6-section5-6.html#section5-6" id="toc-section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5.7-section5-7.html#section5-7" id="toc-section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5.8-section5-8.html#section5-8" id="toc-section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5.9-section5-9.html#section5-9" id="toc-section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5.10-section5-10.html#section5-10" id="toc-section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5.11-section5-11.html#section5-11" id="toc-section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5.12-section5-12.html#section5-12" id="toc-section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5.13-section5-13.html#section5-13" id="toc-section5-13"><span class="toc-section-number">5.13</span> Summary of important R code</a></li>
<li><a href="5.14-section5-14.html#section5-14" id="toc-section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6" id="toc-chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a>
<ul>
<li><a href="6.1-section6-1.html#section6-1" id="toc-section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6.2-section6-6.html#section6-6" id="toc-section6-6"><span class="toc-section-number">6.2</span> Describing relationships with a regression model</a></li>
<li><a href="6.3-section6-7.html#section6-7" id="toc-section6-7"><span class="toc-section-number">6.3</span> Least Squares Estimation</a></li>
<li><a href="6.4-section6-8.html#section6-8" id="toc-section6-8"><span class="toc-section-number">6.4</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6.5-section6-9.html#section6-9" id="toc-section6-9"><span class="toc-section-number">6.5</span> Outliers: leverage and influence</a></li>
<li><a href="6.6-section6-10.html#section6-10" id="toc-section6-10"><span class="toc-section-number">6.6</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6.7-section6-11.html#section6-11" id="toc-section6-11"><span class="toc-section-number">6.7</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6.8-section6-12.html#section6-12" id="toc-section6-12"><span class="toc-section-number">6.8</span> Chapter summary</a></li>
<li><a href="6.9-section6-13.html#section6-13" id="toc-section6-13"><span class="toc-section-number">6.9</span> Summary of important R code</a></li>
<li><a href="6.10-section6-14.html#section6-14" id="toc-section6-14"><span class="toc-section-number">6.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7" id="toc-chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a>
<ul>
<li><a href="7.1-section7-1.html#section7-1" id="toc-section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7.2-section7-2.html#section7-2" id="toc-section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7.3-section7-3.html#section7-3" id="toc-section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7.4-section7-4.html#section7-4" id="toc-section7-4"><span class="toc-section-number">7.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="7.5-section7-5.html#section7-5" id="toc-section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7.6-section7-6.html#section7-6" id="toc-section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7.7-section7-7.html#section7-7" id="toc-section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7.8-section7-8.html#section7-8" id="toc-section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7.9-section7-9.html#section7-9" id="toc-section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7.10-section7-10.html#section7-10" id="toc-section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8" id="toc-chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a>
<ul>
<li><a href="8.1-section8-1.html#section8-1" id="toc-section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8.2-section8-2.html#section8-2" id="toc-section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8.3-section8-3.html#section8-3" id="toc-section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8.4-section8-4.html#section8-4" id="toc-section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8.5-section8-5.html#section8-5" id="toc-section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8.6-section8-6.html#section8-6" id="toc-section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8.7-section8-7.html#section8-7" id="toc-section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8.8-section8-8.html#section8-8" id="toc-section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8.9-section8-9.html#section8-9" id="toc-section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8.10-section8-10.html#section8-10" id="toc-section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8.11-section8-11.html#section8-11" id="toc-section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8.12-section8-12.html#section8-12" id="toc-section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8.13-section8-13.html#section8-13" id="toc-section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8.14-section8-14.html#section8-14" id="toc-section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8.15-section8-15.html#section8-15" id="toc-section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8.16-section8-16.html#section8-16" id="toc-section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8.17-section8-17.html#section8-17" id="toc-section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9" id="toc-chapter9"><span class="toc-section-number">9</span> Case studies</a>
<ul>
<li><a href="9.1-section9-1.html#section9-1" id="toc-section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9.2-section9-2.html#section9-2" id="toc-section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9.3-section9-3.html#section9-3" id="toc-section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9.4-section9-4.html#section9-4" id="toc-section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9.5-section9-5.html#section9-5" id="toc-section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9.6-section9-6.html#section9-6" id="toc-section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section6-1" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Relationships between two quantitative variables</h2>
<p>The independence test in Chapter <a href="5-chapter5.html#chapter5">5</a> provided a technique for assessing evidence of a
relationship between two categorical variables. The terms <strong><em>relationship</em></strong> and <strong><em>association</em></strong>
are synonyms that, in statistics, imply that particular values on one variable tend to occur more often with
some other values of the other variable or that knowing
something about the level of one variable provides information about the
patterns of values on the other variable. These terms are not specific to the
“form” of the relationship – any pattern (strong or weak, negative or positive,
easily described or complicated) satisfy the definition. There are two other
aspects to using these terms in a statistical context. First, they are not
directional – an association between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is the same as saying there is an
association between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>. Second, they are not causal unless the levels of
one of the variables are randomly assigned in an experimental context. We add
to this terminology the idea of correlation between variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.
<strong><em>Correlation</em></strong>, in most statistical contexts, is a measure of the specific type
of relationship between the variables: the <strong>linear relationship between two
quantitative variables</strong><a href="#fn108" class="footnote-ref" id="fnref108"><sup>108</sup></a>.
So as we start to review these ideas from your previous statistics course,
remember that associations and relationships are more general than correlations
and it is possible to have no correlation where there is a strong relationship
between variables. “Correlation” is used colloquially as a synonym for
relationship but we will work to reserve it for its more specialized usage here
to refer specifically to the linear relationship. </p>
<p>Assessing and then modeling relationships between quantitative variables drives
the rest of the chapters,
so we should get started with some motivating examples to start to think about
what relationships between quantitative variables “look like”… To motivate
these methods, we will start with a study of the effects of beer consumption on
blood alcohol levels (<em>BAC</em>, in grams of alcohol per deciliter of blood). A group
of <span class="math inline">\(n = 16\)</span> student volunteers at The Ohio State University drank a
randomly assigned number of beers<a href="#fn109" class="footnote-ref" id="fnref109"><sup>109</sup></a>.
Thirty minutes later, a police officer measured their <em>BAC</em>. Your instincts, especially
as well-educated college students with some chemistry knowledge, should inform
you about the direction of this relationship – that there is a <strong><em>positive
relationship</em></strong> between <code>Beers</code> and <code>BAC</code>. In other words, <strong>higher values of
one variable are associated with higher values of the other</strong>. Similarly,
lower values of one are associated with lower values of the other.
In fact there are online calculators that tell you how much your <em>BAC</em> increases
for each extra beer consumed (for example:
<a href="http://www.craftbeer.com/beer-studies/blood-alcohol-content-calculator" class="uri">http://www.craftbeer.com/beer-studies/blood-alcohol-content-calculator</a>
if you plug in 1 beer). The increase
in <span class="math inline">\(y\)</span> (<code>BAC</code>) for a 1 unit increase in <span class="math inline">\(x\)</span> (here, 1 more beer) is an example of a
<strong><em>slope coefficient</em></strong> that is applicable if the relationship between the
variables is linear and something that will be
fundamental in what is called a <strong><em>simple linear regression model</em></strong>.
In a
simple linear regression model (simple means that there is only one explanatory
variable) the slope is the expected change in the mean response for a one unit
increase in the explanatory variable. You could also use the <em>BAC</em> calculator and
the models that we are going to develop to pick a total number of beers you
will consume and get a predicted <em>BAC</em>, which employs the entire equation we will estimate.</p>
<p>Before we get to the specifics of this model and how we measure correlation, we
should graphically explore the relationship between <code>Beers</code> and <code>BAC</code> in a scatterplot.
Figure <a href="6.1-section6-1.html#fig:Figure6-1">6.1</a> shows a <strong><em>scatterplot</em></strong> of the results that display
the expected positive relationship. Scatterplots display the response pairs for
the two quantitative variables with the
explanatory variable on the <span class="math inline">\(x\)</span>-axis and the response variable on the <span class="math inline">\(y\)</span>-axis. The
relationship between <code>Beers</code> and <code>BAC</code> appears to be relatively linear but
there is possibly more variability than one might expect. For example, for
students consuming 5 beers, their <em>BAC</em>s range from 0.05 to 0.10. If you look
at the online <em>BAC</em> calculators, you will see that other factors such as weight,
sex, and beer percent alcohol can impact
the results. We might also be interested in previous alcohol consumption. In
Chapter <a href="8-chapter8.html#chapter8">8</a>, we will learn how to estimate the relationship between
<code>Beers</code> and <code>BAC</code> after correcting or controlling for those “other variables” using
<strong><em>multiple linear regression</em></strong>, where we incorporate more than one
quantitative explanatory variable into the linear model (somewhat like in the
2-Way ANOVA).
Some of this variability might be hard or impossible to explain
regardless of the other variables available and is considered unexplained variation
and goes into the residual errors in our models, just like in the ANOVA models.
To make scatterplots as in Figure <a href="6.1-section6-1.html#fig:Figure6-1">6.1</a>, you could use the base R function <code>plot</code>, but we will want to again access the power of <code>ggplot2</code> so will use <code>geom_point</code> to add the points to the plot at the “x” and “y” coordinates that you provide in <code>aes(x = ..., y = ...)</code>. </p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="6.1-section6-1.html#cb511-1" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb511-2"><a href="6.1-section6-1.html#cb511-2" tabindex="-1"></a>BB <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/beersbac.csv&quot;</span>)</span></code></pre></div>

<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="6.1-section6-1.html#cb512-1" tabindex="-1"></a>BB <span class="sc">|&gt;</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Beers, <span class="at">y =</span> BAC)) <span class="sc">+</span></span>
<span id="cb512-2"><a href="6.1-section6-1.html#cb512-2" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb512-3"><a href="6.1-section6-1.html#cb512-3" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure6-1"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-1-1.png" alt="Scatterplot of Beers consumed versus BAC." width="75%" />
<p class="caption">
Figure 6.1: Scatterplot of <em>Beers</em> consumed versus <em>BAC</em>.
</p>
</div>
<div style="page-break-after: always;"></div>
<p>There are a few general things to look for in scatterplots:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Assess the</strong> <span class="math inline">\(\underline{\textbf{direction of the relationship}}\)</span> – is it
positive or negative?</p></li>
<li><p><strong>Consider the</strong> <span class="math inline">\(\underline{\textbf{strength of the relationship}}\)</span>.
The general idea of assessing strength visually is about how hard or easy it is
to see the pattern. If it is hard to see a pattern, then it is weak. If it is easy to
see, then it is strong.</p></li>
<li><p><strong>Consider the</strong> <span class="math inline">\(\underline{\textbf{linearity of the relationship}}\)</span>. Does it
appear to curve or does it follow a relatively straight line? Curving relationships are
called <strong><em>curvilinear</em></strong> or <strong><em>nonlinear</em></strong> and can be strong or
weak just like linear relationships – it is all about how tightly the
points follow the pattern you identify.</p></li>
<li><p><strong>Check for</strong> <span class="math inline">\(\underline{\textbf{unusual observations -- outliers}}\)</span> – by looking
for points that don’t follow the overall pattern. Being large in <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>
doesn’t mean that the point is an outlier. Being unusual relative to the overall
pattern makes a point an outlier in this setting.</p></li>
<li><p><strong>Check for</strong> <span class="math inline">\(\underline{\textbf{changing variability}}\)</span> in one variable based
on values of the other variable. This will tie into a constant variance assumption
later in the regression models.</p></li>
<li><p><strong>Finally, look for</strong> <span class="math inline">\(\underline{\textbf{distinct groups}}\)</span> in the scatterplot.
This might suggest that observations from two populations, say males and females,
were combined but the relationship between the two quantitative variables
might be different for the two groups.</p></li>
</ol>
<p>Going back to Figure <a href="6.1-section6-1.html#fig:Figure6-1">6.1</a> it appears that there is a
moderately strong linear
relationship between <code>Beers</code> and <code>BAC</code> – not weak but with some variability
around what appears to be a fairly clear to see straight-line relationship. There might even be a
hint of a nonlinear relationship in the higher beer values. There are no clear
outliers because the observation at 9 beers seems to be following the overall
pattern fairly closely. There is little evidence of non-constant variance
mainly because of the limited size of the data set – we’ll check this with
better plots later. And there are no clearly distinct groups in this plot, possibly
because the # of beers was randomly assigned. These data have one more
interesting feature to be noted – that subjects managed to consume 8 or 9
beers. This seems to be a large number. I have never been able to trace this
data set to the original study so it is hard to know if (1) they had this study
approved by a human subjects research review board to make sure it was “safe”,
(2) every subject in the study was able to consume their randomly assigned
amount, and (3) whether subjects were asked to show up to the study with <em>BAC</em>s
of 0. We also don’t know the exact alcohol concentration of the beer consumed or
volume. So while this is a fun example to start these methods with, a better version of this data set would be nice…</p>
<p>In making scatterplots, there is always a choice of a variable for the
<span class="math inline">\(x\)</span>-axis and the <span class="math inline">\(y\)</span>-axis. It is our
convention to put explanatory or independent variables (the ones used to
explain or predict the responses) on the <span class="math inline">\(x\)</span>-axis. In studies where the subjects are
randomly assigned to levels of a variable, this is very clearly an explanatory
variable, and we can go as far as making causal inferences with it. In
observational studies, it can be less clear which variable explains which. In
these cases, make the most reasonable choice based on the observed variables but
remember that, when the direction of relationship is unclear, you could have
switched the axes and thus the implication of which variable is explanatory.</p>
<!-- ## Estimating the correlation coefficient  {#section6-2} -->
<!-- In terms of quantifying relationships between variables, we start with  -->
<!-- the correlation coefficient, a -->
<!-- measure that is the same regardless of your choice of variables as -->
<!-- explanatory or response. We measure the strength and direction of -->
<!-- linear relationships between two quantitative variables using  -->
<!-- ***Pearson's r*** or ***Pearson's Product Moment Correlation Coefficient***. -->
<!-- For those who really like acronyms, Wikipedia even suggests calling it  -->
<!-- the PPMCC. However,  -->
<!-- its use is so ubiquitous that the lower case ***r*** or just "correlation -->
<!-- coefficient" are often sufficient to identify that you have used the PPMCC.  -->
<!-- Some of the extra distinctions arise because there are other ways of measuring -->
<!-- correlations in other situations (for example between two categorical -->
<!-- variables), but we will not consider them here.  -->
<!-- \newpage -->
<!-- \indent The correlation coefficient, ***r***, is calculated as -->
<!-- $$r = \frac{1}{n-1}\sum^n_{i = 1}\left(\frac{x_i-\bar{x}}{s_x}\right) -->
<!-- \left(\frac{y_i-\bar{y}}{s_y}\right),$$  -->
<!-- where $s_x$ and $s_y$ are the standard deviations of $x$ and $y$. This  -->
<!-- formula can also be written as -->
<!-- $$r = \frac{1}{n-1}\sum^n_{i = 1}z_{x_i}z_{y_i}$$ -->
<!-- where $z_{x_i}$ is the z-score (observation minus mean divided by  -->
<!-- standard deviation) for the $i^{th}$ observation on $x$ and $z_{y_i}$ -->
<!-- is the z-score for the $i^{th}$ observation on $y$. We won't directly -->
<!-- use this formula, but its contents inform the behavior of ***r***. -->
<!-- First, because it is a sum divided by ($n-1$) it is a bit like -->
<!-- an average -- it combines information across all observations and, like the -->
<!-- mean, is sensitive to outliers. Second, it is a dimension-less measure, meaning -->
<!-- that it has no units attached to it. It is based on z-scores which have units -->
<!-- of standard deviations of $x$ or $y$ so the original units of measurement are -->
<!-- canceled out going into this calculation. This also means that changing the -->
<!-- original units of measurement, say from Fahrenheit to Celsius or from miles to -->
<!-- km for one or the other variable will have no impact on the correlation. Less -->
<!-- obviously, the formula guarantees that ***r*** is between -1 and 1. It will -->
<!-- attain -1 for a perfect negative linear relationship, 1 for a perfect positive -->
<!-- linear relationship, and 0 for no linear relationship. We are being careful -->
<!-- here to say ***linear relationship*** because you can have a strong nonlinear -->
<!-- relationship with a correlation of 0. For example, consider  -->
<!-- Figure \@ref(fig:Figure6-2).  -->
<!-- (ref:fig6-2) Scatterplot of an amusing (and strong) relationship that has $r = 0$. -->
<!-- ```{r Figure6-2,fig.cap = "(ref:fig6-2)",echo = F,warning = F,message = F} -->
<!-- x <- seq(from = 0,to = 20,length.out = 20) -->
<!-- y <- (x-mean(x))^2 -->
<!-- x <- c(x,5,15) -->
<!-- y <- c(y,220,220) -->
<!-- dp1 <- tibble(x,y) -->
<!-- dp1 |> ggplot(aes(x = x, y = y)) +  -->
<!--   geom_point(col = "slateblue", size = 2) +  -->
<!--   theme_test() -->
<!-- ``` -->
<!-- \indent There are some conditions for trusting the results that the  -->
<!-- correlation coefficient provides: -->
<!-- 1. Two quantitative variables measured.  -->
<!--     * This might seem silly, but categorical variables can be coded  -->
<!--     numerically and a meaningless correlation can be estimated if you  -->
<!--     are not careful what you correlate.  -->
<!--     <!-- \newpage -->
<p>–&gt;</p>
<!-- 2. The relationship between the variables is relatively linear. -->
<!--     * If the relationship is nonlinear, the correlation is meaningless since it  -->
<!--     only measures linear relationships  -->
<!--     and can be misleading if applied to a nonlinear relationship. -->
<!-- 3. There should be no outliers. \index{outlier} -->
<!--     * The correlation is very sensitive (technically ***not resistant***)  -->
<!--     to the impacts of certain types of outliers and you should generally  -->
<!--     avoid reporting the correlation when they are present. -->
<!--     \index{resistant!not} -->
<!--     * One option in the presence of outliers is to report the correlation  -->
<!--     with and without outliers to see how they influence the estimated  -->
<!--     correlation.  -->
<!-- \indent The correlation coefficient is dimensionless but larger magnitude values -->
<!-- (closer to -1 OR 1) mean stronger linear relationships. A rough interpretation -->
<!-- scale based on experiences working with correlations follows, but this varies -->
<!-- between fields and types of research and variables measured. It depends on the -->
<!-- levels of correlation researchers become used to obtaining, so can even vary -->
<!-- within fields. Use this scale for the discussing the strength of the linear -->
<!-- relationship until you develop your own experience with typical results in a -->
<!-- particular field and what is expected: -->
<!-- * $\left|\boldsymbol{r}\right|<0.3$: weak linear relationship, -->
<!-- * $0.3 < \left|\boldsymbol{r}\right|<0.7$: moderate linear relationship,  -->
<!-- * $0.7 < \left|\boldsymbol{r}\right|<0.9$: strong linear relationship, and -->
<!-- * $0.9 < \left|\boldsymbol{r}\right|<1.0$: very strong linear relationship. -->
<!-- And again note that this scale only relates to the **linear** aspect of -->
<!-- the relationship between the variables.  -->
<!-- \indent When we have linear relationships between two quantitative variables,  -->
<!-- $x$ and $y$, we can obtain estimated correlations from the ``cor`` -->
<!-- function either using ``y ~ x`` or by running the ``cor`` function^[This  -->
<!-- interface with the ``cor`` function only works after you load the  -->
<!-- ``mosaic`` package.] on the entire data set. When you run the ``cor`` -->
<!-- function on a data set it produces a ***correlation matrix*** which  -->
<!-- contains a matrix of correlations where you can triangulate the  -->
<!-- variables being correlated by the row and column names, noting -->
<!-- that the correlation between a variable and itself is 1. A matrix of -->
<!-- correlations is useful for comparing more than two variables, discussed below. \index{correlation matrix} \index{\texttt{cor()}} -->
<!-- ```{r} -->
<!-- library(mosaic) -->
<!-- cor(BAC ~ Beers, data = BB) -->
<!-- cor(BB) -->
<!-- ``` -->
<!-- Based on either version of using the function, we find that the correlation -->
<!-- between ``Beers`` and ``BAC`` is estimated to be 0.89. This suggests a  -->
<!-- strong linear relationship between the -->
<!-- two variables. Examples are about the only way to build up enough experience to -->
<!-- become skillful in using the correlation coefficient. Some additional -->
<!-- complications arise in more complicated studies as the next example -->
<!-- demonstrates.  -->
<!-- \indent @Gude2009 explored the relationship  -->
<!-- between average summer -->
<!-- temperature (degrees F) and area burned (natural log of hectares^[The  -->
<!-- natural log ($\log_e$ or $\ln$) is used in statistics so much that the -->
<!-- function in R ``log`` actually takes the natural log and if you want a  -->
<!-- $\log_{10}$ you have to use the function ``log10``. When statisticians  -->
<!-- say log we mean natural log.] = log(hectares)) by wildfires in Montana  -->
<!-- from 1985 to 2007. The ***log-transformation*** is often used to reduce  -->
<!-- the impacts of really large observations with -->
<!-- non-negative (strictly greater than 0) variables -->
<!-- (more on ***transformations*** and their impacts on regression models  -->
<!-- in Chapter \@ref(chapter7)). -->
<!-- \index{transformation} -->
<!-- Based on your experiences with the wildfire "season" and before  -->
<!-- analyzing the data, I'm sure -->
<!-- you would assume that summer temperature explains the area burned by wildfires.  -->
<!-- But could it be that more fires are related to having warmer summers? That -->
<!-- second direction is unlikely on a state-wide scale but could apply at a -->
<!-- particular weather station that is near a fire. There is another option -- some -->
<!-- other variable is affecting both variables. For example, drier summers might  -->
<!-- be the real explanatory variable that is related to having both warm summers and lots -->
<!-- of fires. These variables are also being measured over time making them examples -->
<!-- of ***time series***. In -->
<!-- this situation, if there are changes over time, they might be attributed to -->
<!-- climate change. So there are really three relationships to explore with the -->
<!-- variables measured here (remembering that the full story might require -->
<!-- measuring even more!): log-area burned versus temperature, temperature versus -->
<!-- year, and log-area burned versus year. \index{log} \index{log10} \index{\texttt{log()}} \index{time series} -->
<!-- \indent As demonstrated in the following code, with more than two variables, we can use the ``cor`` function on all the  -->
<!-- variables and end up getting a matrix of correlations or, simply, the -->
<!-- ***correlation matrix***. \index{correlation matrix} If you triangulate the row and column labels, that cell provides the correlation between that pair of variables. For example, in the first row (``Year``)  -->
<!-- and the last column (``loghectares``), you can find that the correlation -->
<!-- coefficient is ***r*** = 0.362. Note the symmetry in the matrix around the  -->
<!-- diagonal of 1's -- this further illustrates that correlation between  -->
<!-- $x$ and $y$ does not depend on which variable is viewed as the "response". -->
<!-- The estimated correlation -->
<!-- between ``Temperature`` and ``Year`` is -0.004 and the correlation between -->
<!-- ``loghectares`` (*log-hectares burned*) and ``Temperature`` is 0.81. So  -->
<!-- ``Temperature`` has almost no linear -->
<!-- change over time. And there is a strong linear relationship between  -->
<!-- ``loghectares`` and ``Temperature``. So it appears that temperatures may  -->
<!-- be related to log-area burned but that the trend over time in both is less  -->
<!-- clear (at least the linear trends).  -->
<!-- ```{r eval = F} -->
<!-- mtfires <- read_csv("http://www.math.montana.edu/courses/s217/documents/climateR2.csv") -->
<!-- ``` -->
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="6.1-section6-1.html#cb513-1" tabindex="-1"></a><span class="co"># natural log transformation of area burned</span></span>
<span id="cb513-2"><a href="6.1-section6-1.html#cb513-2" tabindex="-1"></a>mtfires <span class="ot">&lt;-</span> mtfires <span class="sc">|&gt;</span> <span class="fu">mutate</span>(<span class="at">loghectares =</span> <span class="fu">log</span>(hectares))</span></code></pre></div>
<!-- ```{r} -->
<!-- # Cuts the original hectares data so only log-scale version in tibble -->
<!-- mtfiresR <- mtfires |> -->
<!--   select(-hectares) -->
<!-- cor(mtfiresR) -->
<!-- ``` -->
<!-- \indent The correlation matrix alone is misleading -- we need to explore scatterplots  -->
<!-- to check for nonlinear -->
<!-- relationships, outliers, and clustering of observations that may be distorting -->
<!-- the numerical measure of the linear relationship. \index{outlier} The ``ggpairs`` -->
<!-- function from the ``GGally`` package [@R-GGally] combines the numerical  -->
<!-- correlation information and scatterplots in one display^[We will not use the "significance stars" in the plot that display with the estimated correlations. You can ignore them but we will sometimes remove them from the plot by using the more complex code of `ggpairs(upper = list(continuous = GGally::wrap(ggally_cor, stars = F)))`.]. -->
<!-- \index{R packages!\textbf{GGally}} -->
<!-- As in the correlation matrix, you -->
<!-- triangulate the variables for the pairwise relationship. The upper right -->
<!-- panel of Figure \@ref(fig:Figure6-3) displays a correlation of 0.362 for  -->
<!-- ``Year`` and ``loghectares`` and the lower left panel contains the  -->
<!-- scatterplot with ``Year`` on the $x$-axis and ``loghectares`` on the $y$-axis. -->
<!-- The correlation between ``Year`` and ``Temperature`` is really small, both -->
<!-- in magnitude and in display, but appears to be nonlinear (it goes down between -->
<!-- 1985 and 1995 and then goes back up), so the correlation coefficient doesn't -->
<!-- mean much here since it just measures the overall linear relationship. We might -->
<!-- say that this is a moderate strength (moderately "clear") curvilinear -->
<!-- relationship. In terms of the underlying climate process, it suggests a -->
<!-- decrease in summer temperatures between 1985 and 1995 and then an increase in -->
<!-- the second half of the data set.  -->
<!-- (ref:fig6-3) Scatterplot matrix of Montana fires data.  -->
<!-- ```{r Figure6-3,fig.cap = "(ref:fig6-3)"} -->
<!-- library(GGally)  -->
<!-- mtfiresR |> ggpairs() + theme_bw() -->
<!-- ``` -->
<!-- \indent As one more example, the Australian Institute of Sport collected data  -->
<!-- on 102 male and 100 female athletes that are available in the ``ais`` -->
<!-- data set from the ``alr4`` package (@R-alr4, @Weisberg2014). -->
<!-- \index{R packages!\textbf{alr4}} -->
<!-- They measured a  -->
<!-- variety of variables including the athlete's Hematocrit (``Hc``,  -->
<!-- units of percentage of red blood cells in the blood), Body Fat Percentage  -->
<!-- (``Bfat``, units of percentage of total body weight), and height (``Ht``, -->
<!-- units of cm). Eventually we might be interested in predicting ``Hc``  -->
<!-- based on the other variables, but for now the associations are of interest.  -->
<!-- (ref:fig6-4) Scatterplot matrix of athlete data.  -->
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="6.1-section6-1.html#cb514-1" tabindex="-1"></a><span class="fu">library</span>(alr4)</span>
<span id="cb514-2"><a href="6.1-section6-1.html#cb514-2" tabindex="-1"></a><span class="fu">data</span>(ais)</span>
<span id="cb514-3"><a href="6.1-section6-1.html#cb514-3" tabindex="-1"></a><span class="fu">library</span>(tibble)</span>
<span id="cb514-4"><a href="6.1-section6-1.html#cb514-4" tabindex="-1"></a>ais <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ais)</span>
<span id="cb514-5"><a href="6.1-section6-1.html#cb514-5" tabindex="-1"></a>aisR <span class="ot">&lt;-</span> ais <span class="sc">|&gt;</span></span>
<span id="cb514-6"><a href="6.1-section6-1.html#cb514-6" tabindex="-1"></a>  <span class="fu">select</span>(Ht, Hc, Bfat)</span>
<span id="cb514-7"><a href="6.1-section6-1.html#cb514-7" tabindex="-1"></a><span class="fu">summary</span>(aisR)</span></code></pre></div>
<pre><code>##        Ht              Hc             Bfat       
##  Min.   :148.9   Min.   :35.90   Min.   : 5.630  
##  1st Qu.:174.0   1st Qu.:40.60   1st Qu.: 8.545  
##  Median :179.7   Median :43.50   Median :11.650  
##  Mean   :180.1   Mean   :43.09   Mean   :13.507  
##  3rd Qu.:186.2   3rd Qu.:45.58   3rd Qu.:18.080  
##  Max.   :209.4   Max.   :59.70   Max.   :35.520</code></pre>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="6.1-section6-1.html#cb516-1" tabindex="-1"></a>aisR <span class="sc">|&gt;</span> <span class="fu">ggpairs</span>() <span class="sc">+</span> <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure6-4"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-4-1.png" alt="(ref:fig6-4)" width="75%" />
<p class="caption">
Figure 6.2: (ref:fig6-4)
</p>
</div>
<!-- <!-- \newpage -->
<p>–&gt;</p>
<!-- ```{r} -->
<!-- cor(aisR) -->
<!-- ``` -->
<!-- ``Ht`` (*Height*) and ``Hc`` (*Hematocrit*) have a moderate positive  -->
<!-- relationship that may contain a slight nonlinearity. It also contains one -->
<!-- clear outlier for a middle height athlete (around 175 cm) with an ``Hc`` -->
<!-- of close to 60% (a result that is extremely high). One might wonder about  -->
<!-- whether this athlete has been doping or -->
<!-- if that measurement involved a recording error. We should consider removing -->
<!-- that observation to see how our results might change without it impacting the -->
<!-- results. For the relationship between ``Bfat`` (*body fat*) and ``Hc``  -->
<!-- (*hematocrit*), that same high ``Hc`` value is a clear outlier. There is -->
<!-- also a high ``Bfat`` (*body fat*) athlete (35%) with a somewhat low  -->
<!-- ``Hc`` value. This also might be influencing our impressions so we will  -->
<!-- remove both "unusual" values and remake the plot. The two offending -->
<!-- observations were found for individuals numbered 56 and 166 in the data set. To access those observations (and then remove them), we introduce the ``slice`` function that we can apply to a tibble as a way to use the row number to either select (as used here) or remove those rows: \index{\texttt{slice()}} -->
<!-- ```{r} -->
<!-- aisR |> slice(56, 166) -->
<!-- ``` -->
<!-- We can create a reduced version of the data (``aisR2``) using the ``slice`` function to slice "out" the rows we don't want by passing a vector of the rows we don't want to retain with a minus sign in front of each of them, ``slice(-56, -166)``, or as vector of rows with a minus in front of the concatenated (``c(...)``) vector (``slice(-c(56, 166))``), and then remake the plot: \index{\texttt{slice()}} \index{remove rows} -->
<!-- (ref:fig6-5) Scatterplot matrix of athlete data with two potential outliers removed. \index{outlier} -->
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="6.1-section6-1.html#cb517-1" tabindex="-1"></a>aisR2 <span class="ot">&lt;-</span> aisR <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span><span class="dv">56</span>, <span class="sc">-</span><span class="dv">166</span>) <span class="co">#Removes observations in rows 56 and 166</span></span>
<span id="cb517-2"><a href="6.1-section6-1.html#cb517-2" tabindex="-1"></a>aisR2 <span class="sc">|&gt;</span> <span class="fu">ggpairs</span>() <span class="sc">+</span> <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure6-5"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-5-1.png" alt="(ref:fig6-5)" width="75%" />
<p class="caption">
Figure 6.3: (ref:fig6-5)
</p>
</div>
<!-- \indent After removing these two unusual observations, the relationships between  -->
<!-- the variables are more obvious (Figure \@ref(fig:Figure6-5)). There is a  -->
<!-- moderate strength, relatively linear relationship between *Height* and  -->
<!-- *Hematocrit*. There is almost no relationship between *Height* and  -->
<!-- *Body Fat %* $(\boldsymbol{r} = -0.20)$. There is a negative, moderate strength,  -->
<!-- somewhat curvilinear relationship between *Hematocrit* and *Body Fat %* -->
<!-- $(\boldsymbol{r} = -0.54)$. As hematocrit increases initially, the body fat  -->
<!-- percentage decreases but at a certain level (around 45% for ``Hc``), the  -->
<!-- body fat percentage seems to -->
<!-- level off. Interestingly, it ended up that removing those two outliers had only -->
<!-- minor impacts on the estimated correlations -- this will not always be the case.  -->
<!-- \indent Sometimes we want to just be able to focus on the correlations, assuming  -->
<!-- we trust that -->
<!-- the correlation is a reasonable description of the results between the -->
<!-- variables. To make it easier to see patterns of positive and negative -->
<!-- correlations, we can employ a different version of the same display from  -->
<!-- the ``corrplot`` package [@R-corrplot] with the ``corrplot.mixed`` function. \index{\texttt{corrplot.mixed()}} -->
<!-- \index{R packages!\textbf{corrplot}} -->
<!-- In this case  -->
<!-- (Figure \@ref(fig:Figure6-6)), it tells much the same story but also allows  -->
<!-- the viewer to easily distinguish both size and direction and read off the  -->
<!-- numerical correlations if desired. \index{correlation plot} -->
<!-- (ref:fig6-6) Correlation plot of the athlete data with two potential outliers removed. Lighter (orange) circle for positive correlations and black for negative correlations.  -->
<!-- ```{r Figure6-6,fig.cap = "(ref:fig6-6)"} -->
<!-- library(corrplot) -->
<!-- corrplot.mixed(cor(aisR2), upper.col = c("black", "orange"),  -->
<!--                lower.col = c("black", "orange")) -->
<!-- ``` -->
<!-- ## Relationships between variables by groups   {#section6-3} -->
<!-- In assessing the relationship -->
<!-- between variables, incorporating information from a third variable can often -->
<!-- enhance the information gathered by either showing that the relationship -->
<!-- between the first two variables is the same across levels of the other variable -->
<!-- or showing that it differs. When the other variable is categorical (or just can -->
<!-- be made categorical), it can be added to scatterplots, changing the symbols and -->
<!-- colors for the points based on the different groups. These techniques are -->
<!-- especially useful if the categorical variable corresponds to potentially -->
<!-- distinct groups in the responses. In the previous example, the data set was -->
<!-- built with male and female athletes. For some characteristics, the -->
<!-- relationships might be the same for both sexes but for others, there are likely -->
<!-- some physiological differences to consider.  -->
<!-- \indent This set of material is where the ``ggplot2`` methods will really pay -->
<!-- off for us, providing you with an extensive set of tools for visualizing -->
<!-- relationships between two quantitative variables and incorporating information -->
<!-- from other variables. There are three ways to add a categorical variable to a -->
<!-- scatterplot that we will use. The first is to modify the colors, the second is -->
<!-- modify the plotting symbol, and the third is to split the graph into panels or -->
<!-- facets based on the groups of the variable. We usually combine the first two -->
<!-- options to give the reader the best chance of detecting the group differences -->
<!-- using both colors and symbols by groups; we will save faceting for a little -->
<!-- later in the material. In these modifications, we can modify the colors and -->
<!-- symbols based on the levels of categorical variable (say ``groupfactor``) by -->
<!-- adding ``color = groupfactor, shape = groupfactor`` to the `aes()` definition -->
<!-- in the initial ``ggplot`` part of the function or within an aesthetic inside -->
<!-- ``geom_point``. Defining the colors and shape within the ``geom_point`` only is -->
<!-- useful if you want to change colors or symbols for the points in a way that -->
<!-- might differ from the colors and groupings you use for other layers in the plot. -->
<!-- The addition of grouping information in the initial ``ggplot`` aesthetic is -->
<!-- called a "global" aesthetic and will apply to all the following geom's. Defining -->
<!-- the colors or symbols within ``geom_point`` is called a "local" aesthetic and -->
<!-- only applies to that layer of the plot. To enhance visibility of the points in -->
<!-- the scatterplot, we often engage different color palettes, using a version^[The -->
<!-- ``end = 0.7`` is used to avoid the lightest yellow color in the gradient that is -->
<!-- often hard to see.] of the ``viridis`` colors with -->
<!-- ``scale_color_viridis_d(end = 0.7)``. Using these ggplot additions, -->
<!-- Figure \@ref(fig:Figure6-7) displays the *Height* and *Hematocrit* relationship -->
<!-- with information on the sex of the athletes where *sex* was coded 0 for males -->
<!-- and 1 for females, changing both the symbol and color for the groups -- with a -->
<!-- legend to help to understand the plot. \index{\texttt{geom\_point()}} -->
<!-- (ref:fig6-7) Scatterplot of athlete's height and hematocrit by sex of athletes. Males were coded as 0s and females as 1s. -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure6-7"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-7-1.png" alt="(ref:fig6-7)" width="75%" />
<p class="caption">
Figure 6.4: (ref:fig6-7)
</p>
</div>
<!-- ```{r eval = F} -->
<!-- aisR2 <- ais |> -->
<!--   slice(-c(56, 166)) |> -->
<!--   select(Ht, Hc, Bfat, Sex) |> -->
<!--   mutate(Sex = factor(Sex)) -->
<!-- aisR2 |> ggplot(mapping = aes(x = Ht, y = Hc)) + -->
<!--   geom_point(aes(shape = Sex, color = Sex), size = 2.5) + -->
<!--   theme_bw() + -->
<!--   scale_color_viridis_d(end = 0.7) + -->
<!--   labs(title = "Scatterplot of Height vs Hematocrit by Sex") -->
<!-- ``` -->
<!-- \indent Adding the grouping information really changes the impressions of the -->
<!-- relationship between *Height* and *Hematocrit* -- within each sex, there is -->
<!-- little relationship between the two variables. The overall relationship is of -->
<!-- moderate strength and positive but the subgroup relationships are weak at best.  -->
<!-- The overall relationship is created by inappropriately combining two groups -->
<!-- that had different means in both the $x$ and $y$ directions. Men have higher -->
<!-- mean heights and hematocrit values than women and putting them together in one -->
<!-- large group creates the misleading overall relationship^[This is related to what -->
<!-- is called Simpson's paradox, where the overall analysis (ignoring a grouping -->
<!-- variable) leads to a conclusion of a relationship in one direction, but when the -->
<!-- relationship is broken down into subgroups it is in the opposite direction in -->
<!-- each group. \index{Simpson's paradox} This emphasizes the importance of checking -->
<!-- and accounting for differences in groups and the more complex models we are -->
<!-- setting the stage to consider in the coming chapters.].  -->
<!-- \indent To get the correlation coefficients by groups, we can subset the data set using a -->
<!-- logical inquiry on the ``Sex`` variable in the updated ``aisR2`` data set, using  -->
<!-- ``Sex == 0`` in the ``filter`` function to get a tibble with male subjects only and ``Sex == 1`` for the female subjects,  -->
<!-- then running the ``cor`` function on each version of the data set: -->
<!-- ```{r} -->
<!-- cor(Hc ~ Ht, data = aisR2 |> filter(Sex == 0)) #Males only -->
<!-- cor(Hc ~ Ht, data = aisR2 |> filter(Sex == 1)) #Females only -->
<!-- ``` -->
<!-- These results show that $\boldsymbol{r} = -0.05$ for *Height* and *Hematocrit*  -->
<!-- for *males* and $\boldsymbol{r} = 0.03$ for *females*. The first suggests a  -->
<!-- very weak negative linear -->
<!-- relationship and the second suggests a very weak positive linear relationship.  -->
<!-- The correlation when the two groups were combined (and group information was  -->
<!-- ignored!) was that $\boldsymbol{r} = 0.37$. So one -->
<!-- conclusion here is that correlations on data sets that contain groups can be -->
<!-- very misleading (if the groups are ignored). It also emphasizes the importance of exploring for potential -->
<!-- subgroups in the data set -- these two groups were not obvious in the initial -->
<!-- plot, but with added information the real story became clear.  -->
<!-- \indent For the *Body Fat* vs *Hematocrit* results in Figure \@ref(fig:Figure6-8), with -->
<!-- an overall correlation of $\boldsymbol{r} = -0.54$, the subgroup correlations -->
<!-- show weaker relationships that also appear to be in different directions  -->
<!-- ($\boldsymbol{r} = 0.13$ for men and $\boldsymbol{r} = -0.17$ for women). This  -->
<!-- doubly reinforces the dangers of aggregating different groups and -->
<!-- ignoring the group information.  -->
<!-- ```{r} -->
<!-- cor(Hc ~ Bfat, data = aisR2 |> filter(Sex == 0)) #Males only -->
<!-- cor(Hc ~ Bfat, data = aisR2 |> filter(Sex == 1)) #Females only -->
<!-- ``` -->
<!-- (ref:fig6-8) Scatterplot of athlete's body fat and hematocrit by sex of athletes. Males were coded as 0s and females as 1s.  -->
<!-- ```{r Figure6-8,fig.cap = "(ref:fig6-8)",echo = F} -->
<!-- aisR2 |> ggplot(mapping = aes(x = Bfat, y = Hc)) + -->
<!--   geom_point(aes(shape = Sex, color = Sex), size = 2.5) + -->
<!--   theme_bw() + -->
<!--   scale_color_viridis_d(end = 0.7) + -->
<!--   labs(title = "Scatterplot of Body Fat vs Hematocrit by Sex") -->
<!-- ``` -->
<!-- <!-- \newpage -->
<p>–&gt;</p>
<!-- ```{r eval = F} -->
<!-- aisR2 |> ggplot(mapping = aes(x = Bfat, y = Hc)) + -->
<!--   geom_point(aes(shape = Sex, color = Sex), size = 2.5) + -->
<!--   theme_bw() + -->
<!--   scale_color_viridis_d(end = 0.7) + -->
<!--   labs(title = "Scatterplot of Body Fat vs Hematocrit by Sex") -->
<!-- ``` -->
<!-- \indent One final exploration for these data involves the *body fat *and -->
<!-- *height* relationship displayed in Figure \@ref(fig:Figure6-9). This -->
<!-- relationship shows an even greater disparity between overall and subgroup -->
<!-- results. The overall relationship is characterized as a weak negative -->
<!-- relationship $(\boldsymbol{r} = -0.20)$ that is not clearly linear or nonlinear. -->
<!-- The subgroup relationships are both clearly positive with a stronger -->
<!-- relationship for men that might also be nonlinear (for  -->
<!-- the linear relationships $\boldsymbol{r} = 0.45$ for women and -->
<!-- $\boldsymbol{r} = 0.20$ for men). Especially for female athletes, those that are  -->
<!-- taller seem to have higher body fat percentages. This might be related to the -->
<!-- types of sports they compete in (there were 10 in the data set) -- that would be -->
<!-- another categorical variable  -->
<!-- we could incorporate... Both groups also seem to demonstrate slightly more  -->
<!-- variability in *Body Fat* associated with taller athletes (each sort of  -->
<!-- "fans out").  -->
<!-- ```{r} -->
<!-- cor(Bfat ~ Ht, data = aisR2 |> filter(Sex == 0)) #Males only -->
<!-- cor(Bfat ~ Ht, data = aisR2 |> filter(Sex == 1)) #Females only -->
<!-- ``` -->
<!-- (ref:fig6-9) Scatterplot of athlete's body fat and height by sex. -->
<!-- ```{r Figure6-9,fig.cap = "(ref:fig6-9)",echo = F} -->
<!-- aisR2 |> ggplot(mapping = aes(x = Ht, y = Bfat)) + -->
<!--   geom_point(aes(shape = Sex, color = Sex), size = 2.5) + -->
<!--   theme_bw() + -->
<!--   scale_color_viridis_d(end = 0.7) + -->
<!--   labs(title = "Scatterplot of Height vs Body Fat by Sex") -->
<!-- ``` -->
<!-- ```{r eval = F} -->
<!-- aisR2 |> ggplot(mapping = aes(x = Ht, y = Bfat)) + -->
<!--   geom_point(aes(shape = Sex, color = Sex), size = 2.5) + -->
<!--   theme_bw() + -->
<!--   scale_color_viridis_d(end = 0.7) + -->
<!--   labs(title = "Scatterplot of Height vs Body Fat by Sex") -->
<!-- ``` -->
<!-- \indent In each of these situations, the sex of the athletes has the potential -->
<!-- to cause misleading conclusions if ignored. There are two ways that this could -->
<!-- occur -- if we did not measure it then we would have no hope to account for it -->
<!-- OR we could have measured it but not adjusted for it in our results, as was done -->
<!-- initially. We distinguish between these two situations by defining the impacts -->
<!-- of this additional variable as either a confounding or lurking variable: -->
<!-- * ***Confounding variable:*** affects the response variable and is related to the  -->
<!-- explanatory variable. The impacts of a confounding variable on the response  -->
<!-- variable cannot be separated from the impacts of the explanatory variable. -->
<!-- \index{confounding} -->
<!-- * ***Lurking variable:*** a potential confounding variable that is not measured  -->
<!-- and is not considered in the interpretation of the study. -->
<!-- \index{lurking} -->
<!-- Lurking variables show up in studies sometimes due to lack of knowledge of the  -->
<!-- system being studied or a lack of resources to measure these variables. Note -->
<!-- that there may be no satisfying resolution to the confounding variable problem -->
<!-- but that it is better to have measured it and know about it than to have it -->
<!-- remain a lurking variable. -->
<!-- \indent To help think about confounding and lurking variables, consider the following  -->
<!-- situation. On many -->
<!-- highways, such as Highway 93 in Montana and north into Canada, recent -->
<!-- construction efforts have been involved in creating safe passages for animals -->
<!-- by adding fencing and animal crossing structures. These structures both can -->
<!-- improve driver safety, save money from costs associated with animal-vehicle -->
<!-- collisions, and increase connectivity of animal populations. Researchers (such as @Clevenger2005) -->
<!-- involved in these projects are interested in which characteristics of -->
<!-- underpasses lead to the most successful structures, mainly measured by rates of -->
<!-- animal usage (number of times they cross under the road). Crossing structures -->
<!-- are typically made using culverts and those tend to be cylindrical. Researchers -->
<!-- are interested in studying the effect of height and width of crossing structures -->
<!-- on animal usage. Unfortunately, all the tallest structures are also the widest -->
<!-- structures. If animals prefer the tall and wide structures, then there is no -->
<!-- way to know if it is due to the height or width of the structure since they are -->
<!-- confounded. If the researchers had only measured width, then they might assume -->
<!-- that it is the important characteristic of the structures but height could be a -->
<!-- lurking variable that really was the factor related to animal usage of the -->
<!-- structures. This is an example where it may not be possible to design a study -->
<!-- that prevents confounding of the two variables *height* and *width*. If the -->
<!-- researchers could control the height and width of the structures independently, then they -->
<!-- could randomly assign both variables to make sure that some narrow structures -->
<!-- are installed that are tall and some that are short. Additionally, they would -->
<!-- also want to have some wide structures that are short and some are tall. Careful design of studies can prevent confounding of variables if they are -->
<!-- known in advance and it is possible to control them, but in observational -->
<!-- studies the observed combinations of variables are uncontrollable. This is why -->
<!-- we need to employ additional caution in interpreting results from observational -->
<!-- studies. Here that would mean that even if width was found to be a predictor of animal usage, we would likely want to avoid saying that width of the structures caused differences in animal usage.  -->
<!-- ## Inference for the correlation coefficient {#section6-4} -->
<!-- We used bootstrapping briefly in -->
<!-- Chapter \@ref(chapter2) to generate nonparametric confidence intervals based  -->
<!-- on the middle -->
<!-- 95% of the bootstrapped version of the statistic. Remember that bootstrapping -->
<!-- involves sampling *with replacement* -->
<!-- from the data set and creates a distribution centered near the statistic from -->
<!-- the real data set. This also mimics sampling under the alternative as opposed -->
<!-- to sampling under the null as in our permutation approaches. Bootstrapping is -->
<!-- particularly useful for making confidence intervals where the distribution of -->
<!-- the statistic may not follow a named distribution. This is the case for the -->
<!-- correlation coefficient which we will see shortly.  -->
<!-- \index{bootstrap} -->
<!-- \indent The correlation is an interesting -->
<!-- summary but it is also an estimator of a population parameter called $\rho$ -->
<!-- (the symbol rho), which is the ***population correlation coefficient***. When  -->
<!-- $\rho = 1$, we have a perfect positive linear relationship in the population;  -->
<!-- when $\rho = -1$, there is a perfect negative linear relationship in the  -->
<!-- population; and when $\rho = 0$, there is no linear relationship in the  -->
<!-- population. Therefore, to test if there is a -->
<!-- linear relationship between two quantitative variables, we use the null -->
<!-- hypothesis $H_0: \rho = 0$ (tests if the true correlation, $\rho$, is 0 -- no -->
<!-- linear relationship). The alternative hypothesis is that there is some -->
<!-- (positive or negative) relationship between the variables in the population,  -->
<!-- $H_A: \rho \ne 0$. The distribution of the Pearson correlation coefficient  -->
<!-- can be complicated in some situations, so we will use -->
<!-- bootstrapping methods to generate confidence intervals for $\rho$ based on  -->
<!-- repeated random samples with replacement from the original data set. -->
<!-- \index{bootstrap} \index{$\rho$}  -->
<!-- If the $C\%$ -->
<!-- confidence interval contains 0, then we would find little to no evidence against the null -->
<!-- hypothesis since 0 is in the interval of our likely values for $\rho$. If  -->
<!-- the $C\%$ confidence interval does not contain 0, then we would find strong evidence against the null  -->
<!-- hypothesis. Along with its use in testing, it is also interesting to be able to generate a confidence interval for $\rho$ to provide an interval where we are $C\%$ confident that the true parameter lies. -->
<!-- \indent The *beers* and *BAC* example seemed to provide a strong relationship with -->
<!-- $\boldsymbol{r} = 0.89$. As correlations approach -1 or 1, the sampling distribution becomes  -->
<!-- more and more skewed. This certainly shows up in the bootstrap distribution  -->
<!-- that the following code produces (Figure \@ref(fig:Figure6-10)). Remember that -->
<!-- bootstrapping utilizes the ``resample`` function applied to the data set to  -->
<!-- create new realizations of the data set by re-sampling -->
<!-- with replacement from those observations. The bold vertical line in  -->
<!-- Figure \@ref(fig:Figure6-10) corresponds to the estimated correlation -->
<!-- $\boldsymbol{r} = 0.89$ and the distribution contains a noticeable left skew  -->
<!-- with a few much smaller $T^*\text{'s}$ possible in bootstrap samples. The  -->
<!-- $C\%$ confidence interval is found based -->
<!-- on the middle $C\%$ of the distribution or by finding the values that put  -->
<!-- $(100-C)/2$ into each tail of the distribution with the ``qdata`` function.  -->
<!-- ```{r} -->
<!-- Tobs <- cor(BAC ~ Beers, data = BB); Tobs -->
<!-- set.seed(614) -->
<!-- B <- 1000 -->
<!-- Tstar <- matrix(NA, nrow = B) -->
<!-- for (b in (1:B)){ -->
<!--   Tstar[b] <- cor(BAC ~ Beers, data = resample(BB)) -->
<!-- } -->
<!-- quantiles <- qdata(Tstar, c(0.025, 0.975)) #95% Confidence Interval -->
<!-- ``` -->
<!-- <!-- \newpage -->
<p>–&gt;</p>
<!-- (ref:fig6-10) Histogram and density curve of the bootstrap distribution of the correlation coefficient with bold vertical line for observed correlation and dashed lines for bounds for the 95% bootstrap confidence interval. -->
<!-- ```{r Figure6-10,fig.cap = "(ref:fig6-10)"} -->
<!-- quantiles -->
<!-- tibble(Tstar) |> ggplot(aes(x = Tstar)) + -->
<!--   geom_histogram(aes(y = ..ncount..), bins = 15, col = 1, fill = "skyblue", center = 0) + -->
<!--   geom_density(aes(y = ..scaled..)) + -->
<!--   theme_bw() + -->
<!--   labs(y = "Density") + -->
<!--   geom_vline(xintercept = quantiles, col = "blue", lwd = 2, lty = 3) + -->
<!--   geom_vline(xintercept = Tobs, col = "red", lwd = 2) + -->
<!--   stat_bin(aes(y = ..ncount.., label = ..count..), bins = 15, -->
<!--            geom = "text", vjust = -0.75) -->
<!-- ``` -->
<!-- These results tell us that the bootstrap 95% CI is from 0.76 to 0.95 -- we are 95% -->
<!-- confident that the true correlation between *Beers* and *BAC* in all OSU students  -->
<!-- like those that volunteered for this study is between 0.76 and 0.95. Note that -->
<!-- there are no units on the correlation coefficient or in this interpretation of it.  -->
<!-- \indent We can also use this confidence interval to test for a linear -->
<!-- relationship between these variables.  -->
<!-- * $\boldsymbol{H_0:\rho = 0:}$ **There is no linear relationship between *Beers* -->
<!-- and *BAC* in the population.** -->
<!-- * $\boldsymbol{H_A: \rho \ne 0:}$ **There is a linear relationship between -->
<!-- *Beers* and *BAC* in the population.** -->
<!-- The 95% confidence level corresponds to a 5% significance level test and if the 95% CI does not contain 0, you know that the p-value would be less than 0.05 and if it does contain 0 that the p-value would be more than 0.05. The 95% CI is -->
<!-- from 0.76 to 0.95, which does not contain 0, so we find strong evidence^[The interval is "far" from the reference value under the null (0) so this provides at least strong evidence. With using confidence intervals for tests, we really don't know much about the strength of evidence against the null hypothesis but the hypothesis test here is a bit more complicated to construct and understand and we will have to tolerate just having crude information about the p-value to assess strength of evidence.] against the null -->
<!-- hypothesis and conclude that there is -->
<!-- a linear relationship between *Beers* and *BAC* in OSU students. We'll revisit this -->
<!-- example using the upcoming regression tools to explore the potential for more -->
<!-- specific conclusions about this relationship. Note that for these inferences to be -->
<!-- accurate, we need to be able to trust that the sample correlation is reasonable -->
<!-- for characterizing the relationship between these variables along with the assumptions we will discuss below.  -->
<!-- \indent In this situation with randomly assigned levels of $x$ and strong evidence against the null  -->
<!-- hypothesis of no relationship, we can further conclude that changing beer  -->
<!-- consumption **causes** changes in the *BAC*. This is a much stronger conclusion -->
<!-- than we can typically make based on correlation coefficients. Correlations and -->
<!-- scatterplots are enticing for infusing causal interpretations in non-causal -->
<!-- situations. Statistics teachers often repeat the mantra that ***correlation is not causation*** -->
<!-- and that generally applies -- except when there is randomization involved in  -->
<!-- the study. It is rarer for -->
<!-- researchers either to assign, or even to be able to assign, levels of -->
<!-- quantitative variables so correlations should be viewed as non-causal unless -->
<!-- the details of the study suggest otherwise.  -->
<!-- ## Are tree diameters related to tree heights? {#section6-5} -->
<!-- In a study at the Upper Flat Creek -->
<!-- study area in the University of Idaho Experimental Forest, a random sample of  -->
<!-- $n = 336$ trees was selected from the forest, with measurements recorded on Douglas  -->
<!-- Fir, Grand Fir, Western Red -->
<!-- Cedar, and Western Larch trees. The data set called ``ufc`` is available from the  -->
<!-- ``spuRs`` package [@R-spuRs] and  -->
<!-- contains ``dbh.cm`` (tree diameter at 1.37 m from the ground, measured in cm) and  -->
<!-- ``height.m`` (tree height in meters). -->
<!-- \index{R packages!\textbf{spuRs}} -->
<!-- The relationship displayed in  -->
<!-- Figure \@ref(fig:Figure6-11) is positive,  -->
<!-- moderately strong with some curvature and increasing variability as the -->
<!-- diameter increases. There do not appear to be groups in the data set but since -->
<!-- this contains four different types of trees, we would want to revisit this plot -->
<!-- by type of tree. To assist in the linearity assessment, we also add the -->
<!-- ``geom_smooth`` to the plot with an option of ``method = "lm"``, which provides -->
<!-- a straight line to best describe the relationship (more on that line in the -->
<!-- coming sections and chapters). The bands around the line are based on the 95% -->
<!-- confidence intervals we can generate for any x-value and relate to pinning down -->
<!-- the true mean value of the y-variable at that value of the x-variable -- but -->
<!-- only apply if the linear relationship is a good description of the relationship -->
<!-- between the variables (which it is not here!). \index{\texttt{geom\_smooth()}}  -->
<!-- (ref:fig6-11) Scatterplot of tree heights (m) vs tree diameters (cm) with estimated straight line relationship (blue line) and 95% confidence interval (grey band). -->
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="6.1-section6-1.html#cb518-1" tabindex="-1"></a><span class="fu">library</span>(spuRs) <span class="co">#install.packages(&quot;spuRs&quot;)</span></span>
<span id="cb518-2"><a href="6.1-section6-1.html#cb518-2" tabindex="-1"></a><span class="fu">data</span>(ufc)</span>
<span id="cb518-3"><a href="6.1-section6-1.html#cb518-3" tabindex="-1"></a>ufc <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ufc)</span>
<span id="cb518-4"><a href="6.1-section6-1.html#cb518-4" tabindex="-1"></a></span>
<span id="cb518-5"><a href="6.1-section6-1.html#cb518-5" tabindex="-1"></a>ufc <span class="sc">|&gt;</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> dbh.cm, <span class="at">y =</span> height.m)) <span class="sc">+</span></span>
<span id="cb518-6"><a href="6.1-section6-1.html#cb518-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb518-7"><a href="6.1-section6-1.html#cb518-7" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb518-8"><a href="6.1-section6-1.html#cb518-8" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure6-11"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-11-1.png" alt="(ref:fig6-11)" width="75%" />
<p class="caption">
Figure 6.5: (ref:fig6-11)
</p>
</div>
<!-- Of particular interest is an observation with a diameter around 58 cm and a height -->
<!-- of less than 5 m. Observing a tree with a diameter around 60 cm is not unusual -->
<!-- in the data set, but none of the other trees with this diameter had heights -->
<!-- under 15 m. It ends up that the likely outlier is in observation number 168 and -->
<!-- because it is so unusual it likely corresponds to either a damaged tree or a -->
<!-- recording error.  -->
<!-- ```{r} -->
<!-- ufc |> slice(168) -->
<!-- ``` -->
<!-- \indent With the outlier in the data set, the correlation is 0.77 and without it, the -->
<!-- correlation increases to 0.79. The removal does not create a big change because -->
<!-- the data set is relatively large and the *diameter* value is close to the mean of the  -->
<!-- $x\text{'s}$^[Observations at the edge of the $x\text{'s}$ will be called high  -->
<!-- leverage points in Section \@ref(section6-9); this point is a low leverage point  -->
<!-- because it is close to mean of the $x\text{'s}$.] but it has some impact on the  -->
<!-- strength of the correlation.  -->
<!-- ```{r} -->
<!-- cor(dbh.cm ~ height.m, data = ufc) -->
<!-- cor(dbh.cm ~ height.m, data = ufc |> slice(-168)) -->
<!-- ``` -->
<!-- \indent With the outlier included, the bootstrap 95% confidence interval goes from 0.702 to -->
<!-- 0.820 -- we are 95% confident that the true correlation between *diameter* and *height* -->
<!-- in the population of trees is between 0.708 and 0.819. When -->
<!-- the outlier is dropped from the data set, the 95% bootstrap CI is 0.753 to 0.826,  -->
<!-- which shifts the lower endpoint of the interval up, reducing the width of the -->
<!-- interval from 0.111 to 0.073 (Figure \@ref(fig:Figure6-12)). In other words, the uncertainty regarding the -->
<!-- value of the population correlation coefficient is reduced. The reason to -->
<!-- remove the observation is that it is unusual based on the observed pattern,  -->
<!-- which implies an error in data collection or sampling from a population other -->
<!-- than the one used for the other observations and, if the removal is justified,  -->
<!-- it helps us refine our inferences for the population parameter. But measuring -->
<!-- the linear relationship in these data where there is a clear curve violates one of -->
<!-- our assumptions of using these methods -- we'll see some other ways of detecting -->
<!-- this issue in Section \@ref(section6-10) and we'll try to "fix" this example using -->
<!-- transformations in Chapter \@ref(chapter7).  -->
<!-- \newpage -->
<!-- (ref:fig6-12) Bootstrap distributions of the correlation coefficient for the full data set (top) and without potential outlier included (bottom) with observed correlation (bold line) and bounds for the 95% confidence interval (dashed lines). Notice the change in spread of the bootstrap distributions as well as the different centers. -->
<!-- ```{r Figure6-12,fig.cap = "(ref:fig6-12)",echo = T, fig.height=9.5} -->
<!-- Tobs <- cor(dbh.cm ~ height.m, data = ufc); Tobs -->
<!-- set.seed(208) -->
<!-- B <- 1000 -->
<!-- Tstar <- matrix(NA, nrow = B) -->
<!-- for (b in (1:B)){ -->
<!--   Tstar[b] <- cor(dbh.cm ~ height.m, data = resample(ufc)) -->
<!-- } -->
<!-- quantiles <- qdata(Tstar, c(.025, .975)) #95% Confidence Interval -->
<!-- quantiles -->
<!-- p1 <- tibble(Tstar) |>  ggplot(aes(x = Tstar)) + -->
<!--   geom_histogram(aes(y = ..ncount..), bins = 25, col = 1, fill = "skyblue", center = 0) + -->
<!--   geom_density(aes(y = ..scaled..)) + -->
<!--   theme_bw() + -->
<!--   labs(y = "Density", title = "Bootstrap distribution of correlation with all data") + -->
<!--   geom_vline(xintercept = quantiles, col = "blue", lwd = 2, lty = 3) + -->
<!--   geom_vline(xintercept = Tobs, col = "red", lwd = 2) + -->
<!--   stat_bin(aes(y = ..ncount.., label = ..count..), bins = 25, -->
<!--            geom = "text", vjust = -0.75) + -->
<!--   xlim(0.6, 0.85) + -->
<!--   ylim(0, 1.1) -->
<!-- Tobs <- cor(dbh.cm ~ height.m, data = ufc |> slice(-168)); Tobs -->
<!-- Tstar <- matrix(NA, nrow = B) -->
<!-- for (b in (1:B)){ -->
<!--   Tstar[b] <- cor(dbh.cm ~ height.m, data = resample(ufc |> slice(-168))) -->
<!-- } -->
<!-- quantiles <- qdata(Tstar, c(.025, .975)) #95% Confidence Interval -->
<!-- quantiles -->
<!-- p2 <- tibble(Tstar) |>  ggplot(aes(x = Tstar)) + -->
<!--   geom_histogram(aes(y = ..ncount..), bins = 25, col = 1, fill = "skyblue", center = 0) +  -->
<!--   geom_density(aes(y = ..scaled..)) + -->
<!--   theme_bw() + -->
<!--   labs(y = "Density", title = "Bootstrap distribution of correlation without outlier") + -->
<!--   geom_vline(xintercept = quantiles, col = "blue", lwd = 2, lty = 3) + -->
<!--   geom_vline(xintercept = Tobs, col = "red", lwd = 2) + -->
<!--   stat_bin(aes(y = ..ncount.., label = ..count..), bins = 25, -->
<!--            geom = "text", vjust = -0.75) + -->
<!--   xlim(0.6, 0.85) + -->
<!--   ylim(0, 1.1) -->
<!-- grid.arrange(p1, p2, ncol = 1) -->
<!-- ``` -->
<!-- \newpage -->
</div>
<div class="footnotes">
<hr />
<ol start="108">
<li id="fn108"><p>There are measures
of correlation between categorical variables but when
statisticians say correlation they mean correlation of quantitative variables.
If they are discussing correlations of other types, they will make that clear.<a href="6.1-section6-1.html#fnref108" class="footnote-back">↩︎</a></p></li>
<li id="fn109"><p>Some of the details of this study have been lost,
so we will assume that the
subjects were randomly assigned and that a beer means a regular sized can of beer and that
the beer was of regular strength. We don’t know if any of that is actually true.
It would be nice to repeat this study to know more details and possibly have a larger sample size but I doubt if our institutional review board would allow students to drink as much as 9 beers.<a href="6.1-section6-1.html#fnref109" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="6-chapter6.html"><button class="btn btn-default">Previous</button></a>
<a href="6.2-section6-6.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
